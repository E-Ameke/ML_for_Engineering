# -*- coding: utf-8 -*-
"""HW_3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wb81OkrKKaLsPGhU1sH7AtqlCKQgUDNC

ML_HOMEWORK_3<BR>
ELIZABETH AMEKE<BR>
662055975

.

Question 1. <br>Image segmentation is a process to highlight useful regions in images. Use the skimage.io module to load the following image. Afterwards, segment the image into multiple useful regions using the k-means clustering method. The segmented image should highlight, for example, the dashboard, the driverâ€™s arms, cars ahead etc., by grouping similar pixels together. You do not need to split the data into train and test set for this problem.
"""

##Answer:

from google.colab import files
from skimage import io
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

#Uploading the image file
uploaded = files.upload()

#Loading the image
for filename in uploaded.keys():
    image = io.imread(filename)

#Creating figure and subplots
fig, axs = plt.subplots(1, 2, figsize=(12, 6))

# Displaying the original image
axs[0].imshow(image)
axs[0].set_title('Original Image')
axs[0].axis('off')

#Reshaping the image to a 2D array of pixels
w, h, d = original_shape = tuple(image.shape)
assert d == 3  # Ensure it's a color image
image_2d = image.reshape((w * h, d))

#Applying k-means clustering
kmeans = KMeans(n_clusters=5, random_state=0)
kmeans.fit(image_2d)
segmented_img_1d = kmeans.labels_
segmented_img = segmented_img_1d.reshape(w, h)

#Displaying the segmented image
axs[1].imshow(segmented_img, cmap='viridis')
axs[1].set_title('Segmented Image')
axs[1].axis('off')

# Show the plot
plt.tight_layout()
plt.show()

from sklearn.cluster import KMeans
from matplotlib import pyplot as plt
from sklearn.metrics import silhouette_score

silhouette_scores = []
for k in range(2, 10):
    kmeans = KMeans(n_clusters=k,init='k-means++',
    n_init='auto',)
    kmeans.fit(image_2d)
    silhouette_scores.append(silhouette_score(image_2d, kmeans.labels_))

fig = plt.figure(figsize=(15, 5))
plt.plot(range(2, 10), silhouette_scores)
plt.grid(True)
plt.xticks(ticks = range(2, 10))
plt.title('silhouette_scores')
plt.xlabel('k')
plt.ylabel('silhouette scores')

"""From my plot above, the k-values between 2 and 5 give good silhouette scores.

.

Question 2. <br>Implement your own linear machine learning model optimized with mini batch gradient descent method to predict the price of a house in a city with population of 160, 000. Train the model to fit the housing prices dataset found on LMS. Vary the batch size from 1,5,10, and 20. Plot the objective function, J for each batch size. You do not need to split the data into train and test set for this problem. What happens when you use batch size equal to one?
"""

##Anwer:

import numpy as np

# Defining the dataset
data_str = """
6.1101,17.592
5.5277,9.1302
8.5186,13.662
7.0032,11.854
5.8598,6.8233
8.3829,11.886
7.4764,4.3483
8.5781,12
6.4862,6.5987
5.0546,3.8166
5.7107,3.2522
14.164,15.505
5.734,3.1551
8.4084,7.2258
5.6407,0.71618
5.3794,3.5129
6.3654,5.3048
5.1301,0.56077
6.4296,3.6518
7.0708,5.3893
6.1891,3.1386
20.27,21.767
5.4901,4.263
6.3261,5.1875
5.5649,3.0825
18.945,22.638
12.828,13.501
10.957,7.0467
13.176,14.692
22.203,24.147
5.2524,-1.22
6.5894,5.9966
9.2482,12.134
5.8918,1.8495
8.2111,6.5426
7.9334,4.5623
8.0959,4.1164
5.6063,3.3928
12.836,10.117
6.3534,5.4974
5.4069,0.55657
6.8825,3.9115
11.708,5.3854
5.7737,2.4406
7.8247,6.7318
7.0931,1.0463
5.0702,5.1337
5.8014,1.844
11.7,8.0043
5.5416,1.0179
7.5402,6.7504
5.3077,1.8396
7.4239,4.2885
7.6031,4.9981
6.3328,1.4233
6.3589,-1.4211
6.2742,2.4756
5.6397,4.6042
9.3102,3.9624
9.4536,5.4141
8.8254,5.1694
5.1793,-0.74279
21.279,17.929
14.908,12.054
18.959,17.054
7.2182,4.8852
8.2951,5.7442
10.236,7.7754
5.4994,1.0173
20.341,20.992
10.136,6.6799
7.3345,4.0259
6.0062,1.2784
7.2259,3.3411
5.0269,-2.6807
6.5479,0.29678
7.5386,3.8845
5.0365,5.7014
10.274,6.7526
5.1077,2.0576
5.7292,0.47953
5.1884,0.20421
6.3557,0.67861
9.7687,7.5435
6.5159,5.3436
8.5172,4.2415
9.1802,6.7981
6.002,0.92695
5.5204,0.152
5.0594,2.8214
5.7077,1.8451
7.6366,4.2959
5.8707,7.2029
5.3054,1.9869
8.2934,0.14454
13.394,9.0551
5.4369,0.61705
"""

# Converting the string data to a numpy array
data = np.array([list(map(float, row.split(','))) for row in data_str.strip().split('\n')])

# Separating X and y
X = data[:, 0].reshape(-1, 1)  # Population of City in 10,000s
y = data[:, 1]  # Price in $10,000s

# Printing the first few entries to verify
print("X (Population of City in 10,000s):\n", X[:5])
print("\ny (Price in $10,000s):\n", y[:5])

import matplotlib.pyplot as plt

# Defining the linear regression model
class LinearRegression:
    def __init__(self, learning_rate=0.01, num_iterations=300):
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations
        self.theta = None
        self.cost_history = []

    def fit(self, X, y, batch_size=1):
        num_samples, num_features = X.shape
        self.theta = np.zeros(num_features)

        for _ in range(self.num_iterations):
            # Randomly shuffling the data
            idx = np.random.permutation(num_samples)
            X_shuffled = X[idx]
            y_shuffled = y[idx]

            # Mini-batch gradient descent
            for i in range(0, num_samples, batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]
                gradient = self.compute_gradient(X_batch, y_batch)
                self.theta -= self.learning_rate * gradient

            # Computing and storing the cost
            cost = self.compute_cost(X, y)
            self.cost_history.append(cost)

    def compute_cost(self, X, y):
        predictions = np.dot(X, self.theta)
        errors = predictions - y
        cost = (1 / (2 * len(y))) * np.sum(errors ** 2)
        return cost

    def compute_gradient(self, X, y):
        predictions = np.dot(X, self.theta)
        errors = predictions - y
        gradient = (1 / len(y)) * np.dot(X.T, errors)
        return gradient

# Loading the housing prices dataset (Assuming you have it stored as X, y)
# X contains the features (population of the city)
# y contains the target variable (house prices)

# Defining the batch sizes to experiment with
batch_sizes = [1, 5, 10, 20]

# Creating subplots for each batch size
fig, axs = plt.subplots(1, len(batch_sizes), figsize=(15, 5))

# Training the model with different batch sizes and plotting side by side
for i, batch_size in enumerate(batch_sizes):
    model = LinearRegression()
    model.fit(X, y, batch_size=batch_size)
    axs[i].plot(model.cost_history, label=f'Batch Size: {batch_size}')
    axs[i].set_xlabel('Iterations')
    axs[i].set_ylabel('Cost (J)')
    axs[i].set_title(f'Batch Size: {batch_size}')
    axs[i].legend()

plt.tight_layout()
plt.show()

""".

Merging all four batch sizes into one plot;
"""

# Defining the linear regression model
class LinearRegression:
    def __init__(self, learning_rate=0.01, num_iterations=300):
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations
        self.theta = None
        self.cost_history = []

    def fit(self, X, y, batch_size=1):
        num_samples, num_features = X.shape
        self.theta = np.zeros(num_features)

        for _ in range(self.num_iterations):
            # Randomly shuffling the data
            idx = np.random.permutation(num_samples)
            X_shuffled = X[idx]
            y_shuffled = y[idx]

            # Mini-batch gradient descent
            for i in range(0, num_samples, batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]
                gradient = self.compute_gradient(X_batch, y_batch)
                self.theta -= self.learning_rate * gradient

            # Computing and storing the cost
            cost = self.compute_cost(X, y)
            self.cost_history.append(cost)

    def compute_cost(self, X, y):
        predictions = np.dot(X, self.theta)
        errors = predictions - y
        cost = (1 / (2 * len(y))) * np.sum(errors ** 2)
        return cost

    def compute_gradient(self, X, y):
        predictions = np.dot(X, self.theta)
        errors = predictions - y
        gradient = (1 / len(y)) * np.dot(X.T, errors)
        return gradient


# Defining the batch sizes to experiment with
batch_sizes = [1, 5, 10, 20]

# Creating a single plot for all batch sizes
plt.figure(figsize=(10, 6))

# Training the model with different batch sizes and plot on the same figure
for batch_size in batch_sizes:
    model = LinearRegression()
    model.fit(X, y, batch_size=batch_size)
    plt.plot(model.cost_history, label=f'Batch Size: {batch_size}')

plt.xlabel('Iterations')
plt.ylabel('Cost (J)')
plt.title('Objective Function (Cost) vs. Iterations for Different Batch Sizes')
plt.legend()
plt.show()

"""Considering the plot above, having a batch size of 1 gives the highest erro (has the highest peak values) of the four batches, thus making the variance a function of the iterations."""

import numpy as np

# Setting a random seed for reproducibility
np.random.seed(20)  # You can use any integer value as the seed

# Instantiat ingthe LinearRegression class
model = LinearRegression()

# Fitting the model to the training data with a chosen batch size
batch_size = 1  # Choose an appropriate batch size
model.fit(X, y, batch_size=batch_size)

# Predicting the price of a house with a population of 160,000
population_of_city = 160000
house_price_prediction = model.theta[0] * population_of_city  # Assuming only one feature (population)
print("Predicted price of a house in a city with a population of 160,000:", house_price_prediction)

"""Predicted price of a house in a city with a population of 160,000 and with a random seed of 20 is 104702.67440763902

.

.

Question 3. <br>Use the Scikit-learn breast cancer Wisconsin dataset and a logistic regression
model to classify breast cancers. You must use a cross-validation method to progressively
reduce the number of features and find the best two features to perform the classification.
Evaluate the model using various classification metrics and report your findings. Use a
70%-30% split.
"""

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFECV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Loading the Breast Cancer Wisconsin dataset
data = load_breast_cancer()
X, y = data.data, data.target

# Splitting the data into 70%-30% train-test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initializing the logistic regression model
log_reg = LogisticRegression(max_iter=10000)

# Performing Recursive Feature Elimination with Cross-Validation (RFECV) for selecting features
rfecv = RFECV(estimator=log_reg, step=1, cv=5, scoring='accuracy')
rfecv.fit(X_train, y_train)

# Getting the indices of the top two features
top_two_indices = rfecv.ranking_.argsort()[:2]

# Getting the best features
best_features = [data.feature_names[i] for i in top_two_indices]

# Training logistic regression model using the best features
X_train_best = X_train[:, top_two_indices]
X_test_best = X_test[:, top_two_indices]
log_reg.fit(X_train_best, y_train)

# Making predictions
y_pred = log_reg.predict(X_test_best)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Evaluation Metrics:")
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

print("\nBest Features Selected by RFECV:")
print(best_features)

# Select the two best features identified by RFECV
feature_indices = [data.feature_names.tolist().index(feature) for feature in ['mean radius', 'worst concave points']]
X_selected = X[:, feature_indices]

# Split the data into 70%-30% train-test sets
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)

# Initialize logistic regression model
log_reg = LogisticRegression(max_iter=10000)

# Train logistic regression model using the best features
log_reg.fit(X_train, y_train)

# Make predictions on the test set
y_pred = log_reg.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print evaluation metrics
print("Evaluation Metrics:")
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

"""The evaluation metrics recorded are accuracy: 0.9239766081871345, <br>precision: 0.905982905982906, recall: 0.9814814814814815, <br>and F1 Score: 0.9422222222222223"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

# Create a meshgrid to plot the decision boundary
x_min, x_max = X_selected[:, 0].min() - 1, X_selected[:, 0].max() + 1
y_min, y_max = X_selected[:, 1].min() - 1, X_selected[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                     np.arange(y_min, y_max, 0.01))

# Create color maps
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00'])

# Plot the decision boundary
Z = log_reg.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.8)

# Plot the training points
scatter = plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold, edgecolor='k', s=20)
plt.xlabel('Mean Radius')
plt.ylabel('Worst Concave Points')
plt.title('Scatter Plot of Breast Cancer Test Data')

# Add legend
classes = ['Benign', 'Malignant']
plt.legend(handles=scatter.legend_elements()[0], labels=classes)

plt.show()

""".

.

Question 4.<br>Construct a neural network with a single hidden layer containing two neurons using Tensorflow. Use ReLU as activation function. Optimize the network with stochastic gradient descent method. Choose mean squared error to calculate the loss. Fit the housing prices dataset found on LMS using the network. Use the trained neural network model to predict the price of a house in a city with population of 165, 000. Calculate a useful
regression metric. Plot the training and validation losses. Use a 70%-30% split for the training and validation dataset. The architecture of the neural network and the optimizer are fixed for this problem. Therefore, you need to choose a suitable learning rate and number of epochs to minimize the loss. Explain the trends you found in the plots for training and validation losses.
"""

##Answer

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Define the dataset
data_str = """
6.1101,17.592
5.5277,9.1302
8.5186,13.662
7.0032,11.854
5.8598,6.8233
8.3829,11.886
7.4764,4.3483
8.5781,12
6.4862,6.5987
5.0546,3.8166
5.7107,3.2522
14.164,15.505
5.734,3.1551
8.4084,7.2258
5.6407,0.71618
5.3794,3.5129
6.3654,5.3048
5.1301,0.56077
6.4296,3.6518
7.0708,5.3893
6.1891,3.1386
20.27,21.767
5.4901,4.263
6.3261,5.1875
5.5649,3.0825
18.945,22.638
12.828,13.501
10.957,7.0467
13.176,14.692
22.203,24.147
5.2524,-1.22
6.5894,5.9966
9.2482,12.134
5.8918,1.8495
8.2111,6.5426
7.9334,4.5623
8.0959,4.1164
5.6063,3.3928
12.836,10.117
6.3534,5.4974
5.4069,0.55657
6.8825,3.9115
11.708,5.3854
5.7737,2.4406
7.8247,6.7318
7.0931,1.0463
5.0702,5.1337
5.8014,1.844
11.7,8.0043
5.5416,1.0179
7.5402,6.7504
5.3077,1.8396
7.4239,4.2885
7.6031,4.9981
6.3328,1.4233
6.3589,-1.4211
6.2742,2.4756
5.6397,4.6042
9.3102,3.9624
9.4536,5.4141
8.8254,5.1694
5.1793,-0.74279
21.279,17.929
14.908,12.054
18.959,17.054
7.2182,4.8852
8.2951,5.7442
10.236,7.7754
5.4994,1.0173
20.341,20.992
10.136,6.6799
7.3345,4.0259
6.0062,1.2784
7.2259,3.3411
5.0269,-2.6807
6.5479,0.29678
7.5386,3.8845
5.0365,5.7014
10.274,6.7526
5.1077,2.0576
5.7292,0.47953
5.1884,0.20421
6.3557,0.67861
9.7687,7.5435
6.5159,5.3436
8.5172,4.2415
9.1802,6.7981
6.002,0.92695
5.5204,0.152
5.0594,2.8214
5.7077,1.8451
7.6366,4.2959
5.8707,7.2029
5.3054,1.9869
8.2934,0.14454
13.394,9.0551
5.4369,0.61705
"""

# Clean the dataset by removing ellipses and any other non-numeric values
clean_data_str = '\n'.join([row for row in data_str.strip().split('\n') if '...' not in row])

# Convert the cleaned string data to a numpy array
data = np.array([list(map(float, row.split(','))) for row in clean_data_str.strip().split('\n')])

# Separate X and y
X = data[:, 0].reshape(-1, 1)  # Population of City in 10,000s
y = data[:, 1]  # Price in $10,000s

# Split the data into training and validation sets (70%-30% split)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)

# Define the neural network architecture
model = tf.keras.Sequential([
    tf.keras.layers.Dense(2, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(1)
])

# Define the optimizer with a small learning rate
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)

# Compile the model with the optimizer and loss function
model.compile(optimizer=optimizer, loss='mean_squared_error')

# Train the model with more epochs
history = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_val, y_val), verbose=0)

# Predict the price of a house in a city with population of 165,000
population = np.array([[165000]])  # Reshape the input to match the shape of X_train
predicted_price = model.predict(population)
print("Predicted price of a house in a city with a population of 165,000:", predicted_price[0][0])

# Plot training and validation losses
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Losses')
plt.legend()
plt.show()

"""The predicted price of a house in a city with a population of 165,000 is $5,955.174

.

From the graph, it can be seen that, at the start of training, there is a rapid decrease in both training and validation losses as the model gains insights from the data. However, as training continues, the rate of loss reduction diminishes and eventually, both the training and validation losses stabilize, converging to a specific value.

.
"""